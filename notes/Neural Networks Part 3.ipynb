{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Part 3\n",
    "\n",
    "*Original course notes [here](https://cs231n.github.io/neural-networks-3/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning: Best Practices and What to Look Out For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the **centered formula** for numerical gradient\n",
    "    - centered formula has an error term of $O(h^2)$, whereas traditional formula has $O(h)$ (can be shown via Taylor expansion of $f(x+h)$ and $f(x-h)$)\n",
    "    - set h to ~1e-5\n",
    "\n",
    "2. Use **relative error** during gradient check to compute a more statistically meaningful threshold for determining whether the analytical and numerical values are the \"same\"\n",
    "\n",
    "3. Use **double precision (64-bit)** floats during error check\n",
    "\n",
    "4. **Scale** the relative error to within the \"active\" range of floating point\n",
    "    - the absolute value difference between the numerical and analytical gradient should not be smaller than about 1e-10\n",
    "    - this is because it is common to normalize the loss over the entire batch (by dividing by batch size, for example)\n",
    "    - therefore, if the abssolute differences are already very small, further dividing them can cause numerical instability\n",
    "    - if the differences are too small, you should scale them by some $1^n$, where n will allow the float exponent to be 0 (for example, scale 5e-10 by 1e10)\n",
    "\n",
    "5. **Kinks** in the objective function\n",
    "    - kinks are problematic because where the analytical gradient may be 0, the numerical gradient is computed by a secant line which skips over the kink, resulting in a non-zero value\n",
    "        - see `svm.ipynb`, inline Q1\n",
    "    - we can detect when a kink is crossed over by checking if the \"winner\" of the objective changes from f(x+h) to f(x-h)\n",
    "        - for example, if $f_1 = f(x+h) < 0$, then the \"winner\" of $\\text{max}(0, f_1) = 0$.\n",
    "        - if $f_2 = f(x-h) > 0$, the winner of $\\text{max}(0, f_2)$ changes from the first to the second argument, so we know the kink is in between $x-h$ and $x+h$\n",
    "\n",
    "6. Gradient check with **fewer datapoints**\n",
    "    - this speeds up the computation time\n",
    "    - it also makes it less likely that you'll run through the kink, since you are checking fewer points along the function\n",
    "\n",
    "7. **Smaller h is not always better**\n",
    "    - too small a value for h can cause numeric instability\n",
    "    - also, increasing or decreasing h by an order of magnitude may result in very different gradients being computed\n",
    "        - visually, similar to issues introduced by kinks, depending on the function, the slope of the secant between x+h and x-h can vary significantly\n",
    "\n",
    "8. Perform gradient check **once loss starts to decrease**\n",
    "    - the specific points for which we check the gradient are usually chosen at random\n",
    "    - a correct gradient at a specific point does not necessarily mean a globally correct analytical implementation\n",
    "    - we should not gradient check at the first iteration, where the weights are initalized randomly\n",
    "        - this is because we could be learning on weights which are not very representative of the data\n",
    "        - for example, if the weights are randomly initialized to very small values, the gradients will be close to 0, and patterns in the gradient could emerge\n",
    "        - the emergence of a pattern doesn't give us much info about whether the gradient was implemented correctly, because an incorrect implementation could produce this pattern\n",
    "\n",
    "9. Check the gradient **without regularization first**\n",
    "    - it is not uncommon for the regularization loss to overwhelm the data loss\n",
    "    - if that happens, we lose information about correctness of the loss gradient, since the regularization gradient is simpler to implement\n",
    "\n",
    "10. **Check regularization loss after data loss**\n",
    "    - this could be done by totally removing data loss\n",
    "    - we could also increase the regularization strength such that the effects of an incorrect implementation become noticeable\n",
    "\n",
    "11. **Turn off augmentations** such as dropout\n",
    "    - non-deterministic augmentations especially will have unpredictable effects on the gradient\n",
    "    - the drawback to turning off augmentations is that we can't gradient check them anymore, e.g. backpropagation of dropout\n",
    "        - this can be mitigated by setting a random seed before computing $f(x+h), f(x-h)$, and the numerical gradient\n",
    "\n",
    "12. **Check a few dimensions from each parameter**\n",
    "    - merging all parameters into a giant vector, then randomly sampling the gradients to be checked, could result in skipping over entire parameters\n",
    "        - this could be the case with bias, for example, which has far fewer terms than weights\n",
    "    - instead, randomly sample dimensions within each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks Before Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Use small initial parameters to check loss**\n",
    "    - set regularization strength to zero during this initial check\n",
    "    - the output should match a pre-computable value that makes sense in the context of the task\n",
    "        - for example, in CIFAR-10, where we have 10 classes, we expect the softmax classifier to about a score close to $-\\log(0.1)$, as explained in `softmax.ipynb`, inline Q1\n",
    "        - notice with small initial weights, the scores should be fairly diffuse as well, which is why setting them to small initial values is important\n",
    "\n",
    "2. **Loss should increase as regularization strength increases**\n",
    "\n",
    "3. **Overfit a tiny subset of data**\n",
    "    - the model should be able to achieve 0 loss on a small (~20) subset of training data\n",
    "    - only once we can overfit this subset should we move on to the entire training set\n",
    "    - note that passing this sanity check does not guarantee a correct implementation\n",
    "        - if there is some bug in either our implementation or the data points we selected, we could be overfitting a bad implementation, which won't generalize to the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks During Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss plots\n",
    "\n",
    "train/val acc plots\n",
    "- heavy overfitting\n",
    "    - increase regularization (more dropout, stronger weight penalty, etc.)\n",
    "    - collect more (and better) data\n",
    "- slight overfitting\n",
    "    - increase capacity by increasing # params\n",
    "\n",
    "ratio of updates:weights\n",
    "- want about -1e-3\n",
    "- higher/lower indicates learning rate might be too high/low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula\n",
    "\n",
    "$$W_t = -\\alpha \\nabla_W{t-1} L$$\n",
    "\n",
    "- update each position by a small step *against* the gradient (since gradient defines steepest *ascent*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inefficiencies with vanilla (stochastic) GD  \n",
    "- for the sake of easier visualization, suppose we are in a 3-D space\n",
    "- from the 3-D \"hills\" defined by our objective function, we have the 2-D elliptical level sets (a level set is just the set of all points that evaluate to a constant)\n",
    "- **note:** the gradient is perpendicular to the level set ([proof](http://mathonline.wikidot.com/the-perpendicularity-of-the-gradient-at-a-point-on-a-level-c))\n",
    "- this is why the gradient can oscillate frequently as it tries to find a minimum, which means more iterations are required to get there\n",
    "- this [article](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_7_Problems.html) explains these intuitive weaknesses of vanilla GD in greater detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physics analogy\n",
    "- we have $F = -\\nabla U = ma$, where $U = mgh$ is the gravitational potential energy\n",
    "- so if we think of our loss function as the potential energy of our ball, then the force experienced by the ball is the negative gradient of the loss function\n",
    "- this makes sense because, in the physics view, we want to maximize the force experienced by the ball, to get it downhill as quickly as possible\n",
    "- now the similarity to SGD is clear\n",
    "- however, we use the gradient to update velocity, instead of position, and then use the new velocity to update position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula\n",
    "$$ v_t = \\beta v_{t-1} + (1-\\beta) \\nabla_{W_{t-1}}L, \\beta \\in [0,1] $$\n",
    "$$ W_t = W_{t-1} + \\alpha v_t $$\n",
    "\n",
    "- $\\beta$ - \"momentum\" (although not literally, as it acts more like a coefficient of friction, proportionally scaling the kinetic energy of the ball as it rolls downhill, i.e. decay rate)\n",
    "    - 0.9 is a good default value; common cross-validation values are [0.5, 0.9, 0.95, 0.99]\n",
    "- $v$ - velocity  \n",
    "- $t$ - iteration\n",
    "- $\\alpha$ - learning rate\n",
    "- $W$ - weight matrix      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding $\\beta$\n",
    "- we can think of $\\beta$ as affecting \"how far back we remember\" \n",
    "- this is because, expanding the recursive relation of $v_t$ (see [derivation](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)), we see that the older a term is, the smaller it becomes, due to exponential scaling\n",
    "\n",
    "$$v_t = \\sum_{i=0}^{t}\\beta^{i}(1-\\beta) \\nabla_{W_{t-i}} L$$\n",
    "\n",
    "- thus each update is computed as a **weighted running sum** of all the previous updates\n",
    "- eventually, terms below a certain threshold will have neglible effect on the update, and are \"forgotten\"\n",
    "- we can also think of $\\beta$ as **how much of the previous velocity we want to retain** from one iteration to the next\n",
    "- when $\\beta = 0$, we just have vanilla GD (i.e., the previous velocity has 0 effect on the update)\n",
    "- when $\\beta = 1$, our initial velocity $v_0$ (assuming $v_0 > 0$) will never decay\n",
    "- notice that for larger $\\beta$, the coefficients do not decrease as quickly, so, intuitively, more of the previous terms have a non-negligble affect on the next update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition for why momentum does better\n",
    "- we explained earlier how SGD was inefficient because the direction per iteration can oscillate rapidly\n",
    "- we can think of momentum as preserving more (compared SGD) of the previous velocity's direction\n",
    "- this way, the change in direction per iteration is not so sharp\n",
    "- the smoother trajectory towards the minimum allows us to skip over other local minima (\"bumps\" along the way)\n",
    "- see [Why Momentum Really Works](https://distill.pub/2017/momentum/) for much more detailed explanation\n",
    "> When $\\beta$ is too small (e.g. in Gradient Descent, $\\beta = 0$), we're over-damping. The particle is immersed in a viscous fluid which saps it of its kinetic energy at every timestep.\n",
    "\n",
    "- this provides better intuition for how momentum improves on SGD, avoiding worse local minima which SGD may converge to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"../images/momentum_update.png\" width=\"300\"/>\n",
    "  <figcaption>Visual interpretation of how momentum changes affects the next step (source: <a href=\"https://cs231n.github.io/neural-networks-3\">Stanford CS231n</a>)\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "In SGD, we would take a step (scaled by learning rate) in the direction of the red vector, the gradient. However, with momentum update, we preserve some of the previous velocity, and use that *add* that to the gradient vector, resulting in a step vector somewhere in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias correction\n",
    "- the result of our formula depends heavily on $v_0$, which we initialize to 0 (this is what is meant by \"bias towards zero\")\n",
    "- the bias is corrected by dividing the momentum by $(1 - \\beta^t)$ after each update (see [StackExchange](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for) and [YouTube](https://youtu.be/-0ZMU-gnm2g) (the explanations are for Adam optimizer, but the logic still applies)\n",
    "- that is, **in expectation**, our estimated velocity will be the same as the true velocity\n",
    "- intuitively, we are converting our running sum into a **running average**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation of momentum update which computes the gradient step starting from the end of the momentum step, rather than computing both gradient and momentum at the starting point. Nesterov momentum theoretically converges more quickly than standard momentum, and has been shown in practice to consistently outperform standard momentum update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula remains the same, except we compute an intermediate \"look-ahead\" position (denoted by $W'$ below) using the previous velocity. We then use this intermediate position to find the updated velocity $v_t$. Finally, we update the position using $v_t$, like we do in regular momentum update.\n",
    "\n",
    "1. Compute temporary momentum-shifted position: $W' = W_{t-1} + \\beta v_{t-1}$\n",
    "2. Nesterov momentum: $v_t = \\beta v_{t-1} + (1-\\beta) \\nabla_{W'} L$\n",
    "3. Update position: $W_t = W_{t-1} - \\alpha v_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Nesterov outperforms standard momentum\n",
    "- if the momentum vector \"overshoots\" (i.e. skips past the minima, possibly ending up in a worse position), computing the gradient at the look-ahead position \"pulls us\" back closer in the direction of the gradient at the original point\n",
    "- this results in a smoother trajectory (i.e. fewer oscillations) since the distance we move less vertically, compared to standard momentum\n",
    "- because of Nesterov momentum's correcting properties, it performs better than standard momentum for larger learning rates; the correcting also allows for larger $\\beta$, which means we can update the loss farther horizontally, *and* have fewer oscillations\n",
    "- note that for smaller learning rates, their performance is theoretically equal ([Sutskever1 et al., Theorem 2.1](https://www.cs.toronto.edu/~fritz/absps/momentum.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"../images/compare_momentum.png\"/>\n",
    "  <figcaption>Nesterov momentum leads to less severe overshooting, resulting in faster convergence (source: <a href=\"https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12\">Akshay L Chandra\n",
    "</a>)\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we want learning rate to be higher at the beginning so we can avoid getting stuck in bad local minima, and also converge more quickly to a good minima\n",
    "- once we approach a good minima, we want to anneal (decrease) the learning rate so that we don't have to perform as many oscillations to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step decay\n",
    "- reduce learning rate by some factor every few epochs\n",
    "- in practice, you can train the model with a fixed learning rate, until the validation loss starts to decrease, at which point you reduce it by some factor\n",
    "\n",
    "Exponential decay\n",
    "- $\\alpha = \\alpha_0 e^{-kt}$\n",
    "- hyperparameters: $\\alpha_0$ and $k$\n",
    "- $t$ is iteration number (can also be in terms of epochs)\n",
    "\n",
    "1/t decay\n",
    "- $\\alpha = \\alpha_0/(1+kt)$\n",
    "- same notation as exponential decay\n",
    "\n",
    "In practice, step decay is usually preferred, because its hyperparameters (the number steps before annealing, and the fraction of decay) are more intuitive than $k$. Also, decay on the slower side if you can afford the computational demand, so that you can train longer.\n",
    "\n",
    "See [machinelearningmastery.com](https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/) for good practices and explanation of more heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula\n",
    "\n",
    "$$ W_t = W_{t-1} - [HL]^{-1} \\nabla_{W_{t-1}} L$$\n",
    "\n",
    "- see: [Why do we need to find the inverse of a Hessian in second order optimization?](https://math.stackexchange.com/q/2710463/155812)\n",
    "\n",
    "Intuition\n",
    "\n",
    "The Hessian matrix $HL$ is the $N \\times N$ Jacobian of the gradient, where $N$ is the number of weights. We can think of each column of the Hessian as the amount of influence each weight has on the gradient. Since the Hessian is computed using second-order derivatives, it describes the local curvature in greater detail. Whereas the first derivative (the gradient) tells us the direction of steepest descent, the second derivative, intuitively, tells us how steep the slope is.\n",
    "\n",
    "This excellent [blog post](https://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/) describes the mathematical intuition, with connections to linear algebra and the geometrical interpretation of this method.\n",
    "\n",
    "Pros\n",
    "\n",
    "Roughly speaking, Newton's method allows us to take greater steps when the slope is shallow (to prevent slow convergence and getting stuck at bad local minima), and smaller steps when the slope is steep (to prevent overshooting). This allows for **faster convergence**. Notice also that the function does not depend on learning rate, so we are dealing with **fewer hyperparameters** as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons\n",
    "- computing the Hessian, if it exists, is extremely expensive, $O(N^2)$ in space and time\n",
    "- computing the second derivative analytically is difficult and may even be intractable\n",
    "- current methods do not work well for mini-batching, thus require training over entire batches, further adding to computation cost\n",
    "- current methods are sensitive to noise and stochastic functions (so augmentations such as dropout must be disabled during training)\n",
    "\n",
    "See: (i) [Karpathy's brief explanation on the impracticalities of second-order methods](https://youtu.be/hd_KFJ5ktUc?t=1980), (ii) [Why second order SGD convergence methods are unpopular for deep learning?](https://stats.stackexchange.com/a/394108/291688), (iii) [Why is Newton's method not widely used in machine learning?](https://stats.stackexchange.com/q/253632/291688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-parameter Adaptive Learning Rate Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section on annealing learning rate, an ideal learning rate is one that decreases over time, skipping over worse local minima, and then slows down towards a good solution to allow for converging. Furthermore, tuning learning rate is an expensive process, so finding a function that can dynamically update the learning rate during training is appealing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formula**\n",
    "\n",
    "1. Store the per-parameter running squared loss:\n",
    "$$s_t = s_{t-1} + (\\nabla_{W_{t-1}} L)^2$$\n",
    "\n",
    "\n",
    "2. Normalize gradient descent with that sum:\n",
    "$$W_t = W_{t-1} + \\frac{\\alpha \\nabla_W L}{\\sqrt{s_t} + \\epsilon}$$\n",
    "\n",
    "Note: We omit the per-parameter index for simplicity, but each $W_t$ is really $W_t^{[i]}$, for the $i$-th weight.\n",
    "\n",
    "Note: $\\epsilon$ is for numerical stability, to avoid division by 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**\n",
    "\n",
    "A particular weight which would have normally received frequent or large updates will have that update scaled down due to the increased denominator. On the other hand, a weight which would received small or infrequent updates won't be scaled as harshly. This helps to both reduce overshooting and quicken convergence, and the convergence of each parameter is independent, making it well-suited for sparsely distributed parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawback**\n",
    "\n",
    "The monotonicity of the denominator means, after enough steps, the learning rates will be scaled down to negligble values. This can lead to learning rates which converge too quickly, with near-zero parameter gradients early in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formula**\n",
    "\n",
    "1. Store the per-parameter (decayed) running squared gradient:\n",
    "\n",
    "$$s_t = \\beta s_{t-1} + (1-\\beta)(\\nabla_{W_{t-1}} L)^2$$\n",
    "\n",
    "\n",
    "2. Same as Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "\n",
    "RMSprop attempts to reduce the aggressive monotonicity of Adagrad by introducing a decay rate, similar to the momentum parameter $\\beta$. This changes the running sum into an exponentially weighted sum (we can also interpret this as a **moving average** between the previous sum and the previous gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvement Over Adagrad**\n",
    "\n",
    "The $s_t$ term does not increase at the same rate as in Adagrad, i.e., the learning rates will not be reduced as quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formula**\n",
    "\n",
    "1. Compute running average of gradient:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla_{W_{t-1}}L$$\n",
    "\n",
    "\n",
    "2. Compute running average of squared gradient:\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla_{W_{t-1}} L)^2$$\n",
    "\n",
    "\n",
    "3. Update position as in RMSprop, using averaged gradient instead of raw.\n",
    "\n",
    "$$W_t = m_t + \\frac{\\alpha \\nabla_W L}{\\sqrt{v_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "\n",
    "- the \"corrective\" properties we saw from (Nesterov) momentum also apply to $m_t$\n",
    "    - by using a running average on the first-order gradient, we are able to reduce the effect of noisy data on the updates\n",
    "- in the bias correction section of momentum, we saw that with initial zero terms for $m$ and $v$, our updates will be biased towards zero for the first few updates\n",
    "- similarly for Adam, we apply bias correction so that the expectation of the estimated $m$ and $v$ equal the true expectation\n",
    "- the expectation of $m$ corresponds to the first moment (mean), and the expectation of $v$ corresponds to the second moment (variance)\n",
    "    - this is simply due to the definition of the $n$-th moment of a random variable $X$, which is the expectation of $X^n$\n",
    "- see [Medium article](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) for a more detailed derivation, but the overall logic really is the same as the bias-corrected momentum, since the running average expression is shared for both momentum and Adam\n",
    "- see [StackExchange](https://stats.stackexchange.com/a/366126/291688) for further explanation of the derivation\n",
    "    - for example, we can treat the gradient at each step as constant, assuming the second moment $\\mathbb{E}[g_i^2] = 0$, which also conveniently eliminates the error term $\\zeta = 0$\n",
    "- see section 3 in the [original paper on Adam](https://arxiv.org/pdf/1412.6980v8.pdf), which relates expectation of the moment of the gradient $g$ and of estimator $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Practice**\n",
    "- recommended values\n",
    "    - $\\epsilon = 1\\text{e}{-8}$\n",
    "    - $\\beta_1 = 0.9$\n",
    "    - $\\beta_2 = 0.999$\n",
    "\n",
    "\n",
    "- recommended methods\n",
    "    - Adam (which tends to outperform RMSprop) or Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
